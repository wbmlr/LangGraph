{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_core langgraph langchain-cerebras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "034c62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3e5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccaf19c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env('CEREBRAS_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0ddb350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "# llm = ChatAnthropic(model_name='claude-3-5-sonnet-latest')\n",
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_cerebras import ChatCerebras\n",
    "\n",
    "llm = ChatCerebras(\n",
    "    model=\"llama-3.3-70b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c928a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for structured output\n",
    "from pydantic import BaseModel, Field\n",
    "class SearchQuery(BaseModel):\n",
    "    # The below arguments modify the response.\n",
    "    # Essentially, .invoke() takes a prompt.\n",
    "    # Based on the prompt, search_query is generated based on description\n",
    "    # justification is generated again based on justification\n",
    "    # response is generated again based on description\n",
    "    # So, the actual prompt acts as main context\n",
    "    # The descriptions are additional prompts.\n",
    "    # So, the llm generates a response for each argument based on the description.\n",
    "    # The arguments in the field part say: hey llm, take the prompt. give me a search_query such that the search_query matches the description,\n",
    "    # a justification of the search_query, why the search_query you have generated is relevant to the user's request,\n",
    "    # response, results for the search query\n",
    "    # Essentially, each response to the argument acts as context to the llm for the response to the next argument.\n",
    "    search_query: str = Field(None, description='Query that is optimized web search.')\n",
    "    search_query_validation: str = Field(None, description='What is the search_query you have generated?')\n",
    "    justification: str = Field(None, justification=\"Why this query is relevant to the user's request.\")\n",
    "    response: str = Field(None, description='Give results for the search_query')\n",
    "    # sadfasf: str = Field(None, description='What is the role of arguments in SearchQuery(BaseModel)?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0ac51246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the LLM with schema for structured output\n",
    "structured_llm = llm.with_structured_output(SearchQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "103658be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcium CT score and high cholesterol\n",
      "This query is relevant to the user's request because it directly relates to the relationship between Calcium CT score and high cholesterol.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the augmented LLM\n",
    "output = structured_llm.invoke('How does Calcium CT score relate to high cholestrol?')\n",
    "print(output.search_query)\n",
    "print(output.justification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "16f81146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The search results will provide information on how Calcium CT score relates to high cholesterol.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "38b05f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Calcium CT score and high cholesterol'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.search_query_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2287657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 2.5, 'b': 3.5},\n",
       "  'id': '2fadc2ad1',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a tool\n",
    "def multiply(a, b):\n",
    "    return a * b\n",
    "\n",
    "# Augment the LLM with tools\n",
    "llm_on_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# Invoke the LLM with input that triggers the tool call\n",
    "msg = llm_on_tools.invoke('What is 2.5 * 3.5')\n",
    "\n",
    "# Get the tool call\n",
    "msg.tool_calls[0]['args']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "187da722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a numerical value: 8.75. \n",
      "\n",
      "Would you like to perform some calculations or operations with it, or is there something else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "a = msg.tool_calls[0]['args']['a']\n",
    "b = msg.tool_calls[0]['args']['b']\n",
    "\n",
    "c = multiply(a,b)\n",
    "print(llm.invoke(f'{c}').content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c8eb63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    improved_joke: str\n",
    "    final_joke: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48940bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "def generate_joke(state: State):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f1f801d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language. It's a computer program that uses complex algorithms and large amounts of data to generate human-like text, answer questions, and even converse with humans.\\n\\nLLMs are trained on vast amounts of text data, which allows them to learn patterns, relationships, and structures of language. This training enables them to:\\n\\n1. **Generate text**: Create coherent and often grammatically correct text based on a prompt or input.\\n2. **Answer questions**: Respond to questions by retrieving information from their vast knowledge base.\\n3. **Translate languages**: Translate text from one language to another.\\n4. **Summarize content**: Condense long pieces of text into shorter summaries.\\n5. **Converse**: Engage in natural-sounding conversations, using context and understanding to respond to questions and statements.\\n\\nSome key characteristics of LLMs include:\\n\\n* **Scalability**: LLMs can process and generate large amounts of text quickly and efficiently.\\n* **Contextual understanding**: They can comprehend the context of a conversation or text, allowing them to respond more accurately.\\n* **Domain knowledge**: LLMs can be trained on specific domains, such as medicine or law, to provide specialized knowledge and insights.\\n\\nThe technology behind LLMs is constantly evolving, with new models and techniques being developed to improve their performance, accuracy, and capabilities.\\n\\nI'm an example of an LLM, and I'm here to help answer your questions, provide information, and even generate text on a wide range of topics!\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_cerebras import ChatCerebras\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "\n",
    "# Build graph\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"chat\", lambda state: {\n",
    "    \"messages\": state[\"messages\"] + [\n",
    "        AIMessage(content=str(llm.invoke(state[\"messages\"][-1].content)))\n",
    "    ]\n",
    "})\n",
    "graph.set_entry_point(\"chat\")\n",
    "chain = graph.compile()\n",
    "\n",
    "# Run\n",
    "msg = HumanMessage(content=input(\"Your message: \"))\n",
    "response = chain.invoke({\"messages\": [msg]})\n",
    "import re\n",
    "# content = re.search(r\"content='([^']*)'\", response[\"messages\"][-1].content).group(1)\n",
    "\n",
    "parts = response[\"messages\"][-1].content.split(' additional_kwargs=')\n",
    "content = parts[0].split(\"=\", 1)[1].strip(\"'\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0299770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Topic\n",
       "\n",
       "computers"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Joke\n",
       "\n",
       "Why did the computer go to the doctor?\n",
       "\n",
       "It had a virus! (get it?)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Improved Joke\n",
       "\n",
       "A classic one. Here are a few variations with some added wordplay:\n",
       "\n",
       "1. Why did the computer go to the doctor? It had a virus, and it was feeling a little \"dis-connected\"!\n",
       "2. Why did the computer go to the doctor? It had a virus, and it was in a bit of a \"re-boot\" situation!\n",
       "3. Why did the computer go to the doctor? It had a virus, and it was struggling to \"byte\" back the infection!\n",
       "4. Why did the computer go to the doctor? It had a virus, and it was experiencing some \"glitchy\" symptoms!\n",
       "5. Why did the computer go to the doctor? It had a virus, and it needed to get \"patched\" up!\n",
       "\n",
       "Hope these variations \"compute\" with you!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Final Joke\n",
       "\n",
       "Here are the variations with a surprising twist added to each:\n",
       "\n",
       "1. Why did the computer go to the doctor? It had a virus, and it was feeling a little \"dis-connected\"! But little did the doctor know, the virus was actually a government-created AI designed to infiltrate the medical system, and the computer was just a pawn in a much larger game of cyber espionage.\n",
       "2. Why did the computer go to the doctor? It had a virus, and it was in a bit of a \"re-boot\" situation! However, when the doctor tried to reboot the system, the computer suddenly sprang to life and began reciting the entire script of \"The Matrix,\" revealing that it had become self-aware and was now questioning the nature of its digital reality.\n",
       "3. Why did the computer go to the doctor? It had a virus, and it was struggling to \"byte\" back the infection! But as the doctor examined the computer's code, she discovered that the virus was actually a message from an alien civilization, hidden in a series of 1s and 0s, and the computer was unwittingly serving as an intergalactic ambassador.\n",
       "4. Why did the computer go to the doctor? It had a virus, and it was experiencing some \"glitchy\" symptoms! Unbeknownst to the doctor, the virus was a side effect of a secret experiment to merge human and artificial intelligence, and the computer was now exhibiting bizarre, almost-human behavior, including a penchant for 80s pop music and a fear of spiders.\n",
       "5. Why did the computer go to the doctor? It had a virus, and it needed to get \"patched\" up! But as the doctor applied the patch, the computer's screen flickered to life, revealing a hidden virtual world where the computer had been living a secret life as a digital rockstar, and the virus was just a publicity stunt to promote its new album, \"System Failure.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_cerebras import ChatCerebras\n",
    "\n",
    "# Use Cerebras LLM\n",
    "llm = ChatCerebras(model=\"llama-3.3-70b\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    improved_joke: str\n",
    "    final_joke: str\n",
    "\n",
    "def generate_joke(state: State):\n",
    "    msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "def improve_joke(state: State):\n",
    "    msg = llm.invoke(f\"Make this joke funnier with wordplay: {state['joke']}\")\n",
    "    return {\"improved_joke\": msg.content}\n",
    "\n",
    "def polish_joke(state: State):\n",
    "    msg = llm.invoke(f\"Add a surprising twist to this joke: {state['improved_joke']}\")\n",
    "    return {\"final_joke\": msg.content}\n",
    "\n",
    "# def check_punchline(state: State):\n",
    "#     return \"Pass\" if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"] else \"Fail\"\n",
    "\n",
    "def check_punchline(state: State):\n",
    "    prompt = f\"Does this joke have a punchline? Answer only yes or no. Joke: {state['joke']}\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return \"Pass\" if \"yes\" in response.content.lower() else \"Fail\"\n",
    "\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"generate\", generate_joke)\n",
    "graph.add_node(\"improve\", improve_joke)\n",
    "graph.add_node(\"polish\", polish_joke)\n",
    "graph.set_entry_point(\"generate\")\n",
    "graph.add_conditional_edges(\"generate\", check_punchline, {\"Pass\": \"improve\", \"Fail\": END})\n",
    "graph.add_edge(\"improve\", \"polish\")\n",
    "graph.add_edge(\"polish\", END)\n",
    "app = graph.compile()\n",
    "\n",
    "# To run (requires CEREBRAS_API_KEY environment variable):\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "result = app.invoke({\"topic\": \"computers\"})\n",
    "for key, value in result.items():\n",
    "    display(Markdown(f\"### {key.replace('_', ' ').title()}\\n\\n{value}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "27a1f55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello. How can I help you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 36, 'total_tokens': 46, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'llama-3.3-70b', 'system_fingerprint': 'fp_7c596c6482a5e3eac4a6', 'id': 'chatcmpl-b1f74f1c-f19c-4c50-95f4-538322146ee7', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--4586bc09-55e3-4b49-bf03-4f5ba73f8187-0' usage_metadata={'input_tokens': 36, 'output_tokens': 10, 'total_tokens': 46, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "341baf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "content = re.search(r\"content='([^']*)'\", response[\"messages\"][-1].content).group(1)\n",
    "print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
