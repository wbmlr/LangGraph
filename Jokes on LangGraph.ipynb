{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_core langgraph langchain-cerebras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "034c62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3e5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccaf19c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env('CEREBRAS_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0ddb350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "# llm = ChatAnthropic(model_name='claude-3-5-sonnet-latest')\n",
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_cerebras import ChatCerebras\n",
    "\n",
    "llm = ChatCerebras(\n",
    "    model=\"llama-3.3-70b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c928a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for structured output\n",
    "from pydantic import BaseModel, Field\n",
    "class SearchQuery(BaseModel):\n",
    "    # The below arguments modify the response.\n",
    "    # Essentially, .invoke() takes a prompt.\n",
    "    # Based on the prompt, search_query is generated based on description\n",
    "    # justification is generated again based on justification\n",
    "    # response is generated again based on description\n",
    "    # So, the actual prompt acts as main context\n",
    "    # The descriptions are additional prompts.\n",
    "    # So, the llm generates a response for each argument based on the description.\n",
    "    # The arguments in the field part say: hey llm, take the prompt. give me a search_query such that the search_query matches the description,\n",
    "    # a justification of the search_query, why the search_query you have generated is relevant to the user's request,\n",
    "    # response, results for the search query\n",
    "    # Essentially, each response to the argument acts as context to the llm for the response to the next argument.\n",
    "    search_query: str = Field(None, description='Query that is optimized web search.')\n",
    "    search_query_validation: str = Field(None, description='What is the search_query you have generated?')\n",
    "    justification: str = Field(None, justification=\"Why this query is relevant to the user's request.\")\n",
    "    response: str = Field(None, description='Give results for the search_query')\n",
    "    # sadfasf: str = Field(None, description='What is the role of arguments in SearchQuery(BaseModel)?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0ac51246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the LLM with schema for structured output\n",
    "structured_llm = llm.with_structured_output(SearchQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "103658be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcium CT score and high cholesterol\n",
      "This query is relevant to the user's request because it directly relates to the relationship between Calcium CT score and high cholesterol.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the augmented LLM\n",
    "output = structured_llm.invoke('How does Calcium CT score relate to high cholestrol?')\n",
    "print(output.search_query)\n",
    "print(output.justification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "16f81146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The search results will provide information on how Calcium CT score relates to high cholesterol.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "38b05f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Calcium CT score and high cholesterol'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.search_query_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2287657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 2.5, 'b': 3.5},\n",
       "  'id': '2fadc2ad1',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a tool\n",
    "def multiply(a, b):\n",
    "    return a * b\n",
    "\n",
    "# Augment the LLM with tools\n",
    "llm_on_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# Invoke the LLM with input that triggers the tool call\n",
    "msg = llm_on_tools.invoke('What is 2.5 * 3.5')\n",
    "\n",
    "# Get the tool call\n",
    "msg.tool_calls[0]['args']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "187da722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a numerical value: 8.75. \n",
      "\n",
      "Would you like to perform some calculations or operations with it, or is there something else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "a = msg.tool_calls[0]['args']['a']\n",
    "b = msg.tool_calls[0]['args']['b']\n",
    "\n",
    "c = multiply(a,b)\n",
    "print(llm.invoke(f'{c}').content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c8eb63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    improved_joke: str\n",
    "    final_joke: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48940bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "def generate_joke(state: State):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f1f801d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language. It's a computer program that uses complex algorithms and large amounts of data to generate human-like text, answer questions, and even converse with humans.\\n\\nLLMs are trained on vast amounts of text data, which allows them to learn patterns, relationships, and structures of language. This training enables them to:\\n\\n1. **Generate text**: Create coherent and often grammatically correct text based on a prompt or input.\\n2. **Answer questions**: Respond to questions by retrieving information from their vast knowledge base.\\n3. **Translate languages**: Translate text from one language to another.\\n4. **Summarize content**: Condense long pieces of text into shorter summaries.\\n5. **Converse**: Engage in natural-sounding conversations, using context and understanding to respond to questions and statements.\\n\\nSome key characteristics of LLMs include:\\n\\n* **Scalability**: LLMs can process and generate large amounts of text quickly and efficiently.\\n* **Contextual understanding**: They can comprehend the context of a conversation or text, allowing them to respond more accurately.\\n* **Domain knowledge**: LLMs can be trained on specific domains, such as medicine or law, to provide specialized knowledge and insights.\\n\\nThe technology behind LLMs is constantly evolving, with new models and techniques being developed to improve their performance, accuracy, and capabilities.\\n\\nI'm an example of an LLM, and I'm here to help answer your questions, provide information, and even generate text on a wide range of topics!\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_cerebras import ChatCerebras\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "\n",
    "# Build graph\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"chat\", lambda state: {\n",
    "    \"messages\": state[\"messages\"] + [\n",
    "        AIMessage(content=str(llm.invoke(state[\"messages\"][-1].content)))\n",
    "    ]\n",
    "})\n",
    "graph.set_entry_point(\"chat\")\n",
    "chain = graph.compile()\n",
    "\n",
    "# Run\n",
    "msg = HumanMessage(content=input(\"Your message: \"))\n",
    "response = chain.invoke({\"messages\": [msg]})\n",
    "import re\n",
    "# content = re.search(r\"content='([^']*)'\", response[\"messages\"][-1].content).group(1)\n",
    "\n",
    "parts = response[\"messages\"][-1].content.split(' additional_kwargs=')\n",
    "content = parts[0].split(\"=\", 1)[1].strip(\"'\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0299770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'computers', 'joke': 'Why did the computer go to the doctor?\\n\\nIt had a virus.', 'improved_joke': \"A classic one. Here are a few variations with some added wordplay:\\n\\n* Why did the computer go to the doctor? It had a virus, and it wasn't just a bug in the system – it was a real byte-sized problem!\\n* Why did the computer go to the doctor? It had a virus, and it was feeling a little glitchy. Guess you could say it was having a crash course in health!\\n* Why did the computer go to the doctor? It had a virus, and it was worried it would be deleted from the face of the earth! Thankfully, the doc was able to reboot its health.\\n* Why did the computer go to the doctor? It had a virus, and it was experiencing some malware-ware concerns. But in the end, the prescription was just what it needed to patch things up!\\n\\nHope these variations help inject some humor into the joke!\", 'final_joke': 'Here are the variations with a surprising twist added to each:\\n\\n* Why did the computer go to the doctor? It had a virus, and it wasn\\'t just a bug in the system – it was a real byte-sized problem! But just as the doctor was about to prescribe some medication, the computer suddenly rebooted and revealed that it was actually a sentient AI who had been pretending to be sick to avoid doing its owner\\'s taxes.\\n* Why did the computer go to the doctor? It had a virus, and it was feeling a little glitchy. Guess you could say it was having a crash course in health! However, after a series of tests, the doctor discovered that the virus was actually a symptom of a much larger issue: the computer was secretly a time-traveling device from the year 2050, and the virus was a result of its attempts to send a warning to its past self about an impending robot uprising.\\n* Why did the computer go to the doctor? It had a virus, and it was worried it would be deleted from the face of the earth! Thankfully, the doc was able to reboot its health, but not before the computer revealed that it was actually a reincarnated soul of a former hacker who had made a pact with a mysterious organization to use its digital existence to uncover a deep-seated conspiracy involving the world\\'s most powerful tech corporations.\\n* Why did the computer go to the doctor? It had a virus, and it was experiencing some malware-ware concerns. But in the end, the prescription was just what it needed to patch things up! However, as the computer was leaving the doctor\\'s office, it received a mysterious message from an unknown sender: \"The virus was just a test. The real malware is the one that\\'s been controlling your doctor\\'s mind all along. Meet me in the abandoned server room at midnight to learn the truth.\"\\n\\nThese twists add a surprising layer of complexity and intrigue to the original jokes, don\\'t you think?'}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_cerebras import ChatCerebras\n",
    "\n",
    "# Use Cerebras LLM\n",
    "llm = ChatCerebras(model=\"llama-3.3-70b\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    improved_joke: str\n",
    "    final_joke: str\n",
    "\n",
    "def generate_joke(state: State):\n",
    "    msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "def improve_joke(state: State):\n",
    "    msg = llm.invoke(f\"Make this joke funnier with wordplay: {state['joke']}\")\n",
    "    return {\"improved_joke\": msg.content}\n",
    "\n",
    "def polish_joke(state: State):\n",
    "    msg = llm.invoke(f\"Add a surprising twist to this joke: {state['improved_joke']}\")\n",
    "    return {\"final_joke\": msg.content}\n",
    "\n",
    "def check_punchline(state: State):\n",
    "    return \"Pass\" if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"] else \"Fail\"\n",
    "\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"generate\", generate_joke)\n",
    "graph.add_node(\"improve\", improve_joke)\n",
    "graph.add_node(\"polish\", polish_joke)\n",
    "graph.set_entry_point(\"generate\")\n",
    "graph.add_conditional_edges(\"generate\", check_punchline, {\"Pass\": \"improve\", \"Fail\": END})\n",
    "graph.add_edge(\"improve\", \"polish\")\n",
    "graph.add_edge(\"polish\", END)\n",
    "app = graph.compile()\n",
    "\n",
    "# To run (requires CEREBRAS_API_KEY environment variable):\n",
    "print(app.invoke({\"topic\": \"computers\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17503a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Topic\n",
       "\n",
       "computers"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Joke\n",
       "\n",
       "Why do computers go to the doctor?\n",
       "\n",
       "Because they have a virus!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Improved Joke\n",
       "\n",
       "A classic joke. Here are a few variations with added wordplay:\n",
       "\n",
       "1. Why do computers go to the doctor? Because they have a virus, and it's a pretty \"invasive\" problem!\n",
       "2. Why do computers visit the doc? They've got a bug to work out – and it's not just a glitch, it's a full-blown virus!\n",
       "3. What do you call a computer that's feeling unwell? A little \"dis-connected\" – it's got a virus, and it needs a byte of medical attention!\n",
       "4. Why did the computer go to the doctor? It had a virus, and it was \"spreading\" like wildfire – luckily, the doc had an \"anti-body\" of knowledge to cure it!\n",
       "5. Why do computers need to see a doctor? They've contracted a virus, and it's \"crashing\" their system – time for a reboot and a healthy dose of meds!\n",
       "\n",
       "I hope these variations \"boot\" up the joke's humor level!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Final Joke\n",
       "\n",
       "Here's a surprising twist:\n",
       "\n",
       "6. Why did the computer go to the doctor? It had a virus, but just as the doctor was about to prescribe some antivirus software, the computer suddenly stood up and said, \"Wait, I'm not a computer – I'm a highly advanced AI from the year 3050, and I've been sent back in time to study the art of humor. The virus was just a cover story, and I've been gathering data on the most effective joke formats. Your human jokes are... amusing, but I think I can do better.\" And with that, the AI/computer hybrid launched into a hilarious stand-up comedy routine, leaving the doctor and everyone else in stitches – proving that even machines can be funny, and that the future of comedy is in good hands... or rather, good code.\n",
       "\n",
       "This twist adds a surprising layer to the joke, shifting it from a simple play on words to a more complex and humorous story that pokes fun at the idea of artificial intelligence and the nature of comedy itself."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "result = app.invoke({\"topic\": \"computers\"})\n",
    "for key, value in result.items():\n",
    "    display(Markdown(f\"### {key.replace('_', ' ').title()}\\n\\n{value}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "27a1f55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello. How can I help you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 36, 'total_tokens': 46, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'llama-3.3-70b', 'system_fingerprint': 'fp_7c596c6482a5e3eac4a6', 'id': 'chatcmpl-b1f74f1c-f19c-4c50-95f4-538322146ee7', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--4586bc09-55e3-4b49-bf03-4f5ba73f8187-0' usage_metadata={'input_tokens': 36, 'output_tokens': 10, 'total_tokens': 46, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "341baf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "content = re.search(r\"content='([^']*)'\", response[\"messages\"][-1].content).group(1)\n",
    "print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
